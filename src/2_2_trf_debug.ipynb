{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take a deeper look.\n",
    "import arrayfire as af\n",
    "\n",
    "layer_x = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_x\").to_ndarray()\n",
    "\n",
    "MHAttention_v = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"MHAttention_v\").to_ndarray()\n",
    "MHAttention_result_attn = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"MHAttention_result_attn\").to_ndarray()\n",
    "MHAttention_result_result = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"MHAttention_result_result\").to_ndarray()\n",
    "MHAttention_result_result_2 = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"MHAttention_result_result_2\").to_ndarray()\n",
    "\n",
    "selfAttention_result = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"selfAttention_result\").to_ndarray()\n",
    "selfAttention_wf_ = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"selfAttention_wf_\").to_ndarray()\n",
    "selfAttention_result_2 = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"selfAttention_result_2\").to_ndarray()\n",
    "\n",
    "layer_selfAttnResult = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_selfAttnResult\").to_ndarray()\n",
    "layer_norm1_w = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_norm1_w\").to_ndarray()\n",
    "layer_norm1_b = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_norm1_b\").to_ndarray()\n",
    "layer_h = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_h\").to_ndarray()\n",
    "layer_h_2 = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_h_2\").to_ndarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing again after chaning MCTCSelfOutput to this differnt way flashlight seems to do things\n",
    "\n",
    "# testing after fixing the model & weights\n",
    "import torch\n",
    "from transformers import MCTCForCTC, MCTCProcessor, MCTCConfig\n",
    "config = MCTCConfig()\n",
    "model = MCTCForCTC(config)\n",
    "model.load_state_dict(torch.load(\"./ported_pytorch_model.bin\"))\n",
    "model = model.eval()\n",
    "our_attn = model.mctc.encoder.layers[0].attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT torch.Size([1, 321, 1536]) tensor(330.5734) tensor(0.1587)\n",
      "selfattn_attention_probs torch.Size([1, 4, 321, 321]) tensor(1283.9999, grad_fn=<SumBackward0>)\n",
      "selfattn_attention_probs_HEADS torch.Size([1, 4, 321, 321]) tensor(1283.9999, grad_fn=<SumBackward0>)\n",
      "value_layer torch.Size([1, 4, 321, 384]) tensor(-995.1271, grad_fn=<SumBackward0>) tensor(0.1639, grad_fn=<StdBackward0>)\n",
      "context_layer torch.Size([1, 4, 321, 384]) tensor(-1023.2032, grad_fn=<SumBackward0>) tensor(0.0612, grad_fn=<StdBackward0>)\n",
      "SelfOUtput input torch.Size([1, 321, 1536]) tensor(-1023.2032, grad_fn=<SumBackward0>)\n",
      "SelfOUtput prenorm torch.Size([1, 321, 1536]) tensor(22.9749, grad_fn=<SumBackward0>)\n",
      "SelfOUtput input_tensor torch.Size([1, 321, 1536]) tensor(330.5734)\n",
      "SelfOUtput postnorm torch.Size([1, 321, 1536]) tensor(-618.0341, grad_fn=<SumBackward0>)\n",
      "OUTPUT torch.Size([1, 321, 1536]) tensor(-618.0341, grad_fn=<SumBackward0>) tensor(0.2111, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def valueprint(k, name=None):\n",
    "    print(name, k.shape, k.sum(), k.std())\n",
    "    \n",
    "our_output = torch.Tensor(layer_x).unsqueeze(0).transpose(1,2)\n",
    "valueprint(our_output, \"INPUT\")\n",
    "our_output = our_attn(our_output)[0]\n",
    "valueprint(our_output, \"OUTPUT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None (321, 321, 4) 1283.9999 0.0022644862\n"
     ]
    }
   ],
   "source": [
    "'''1. QKV attention, then relative key position embedding then softmax for final \n",
    "attn values\n",
    "\n",
    "selfattn_attention_probs torch.Size([1, 4, 321, 321]) tensor(1283.9999, grad_fn=<SumBackward0>)\n",
    "'''\n",
    "valueprint(MHAttention_result_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None (321, 384, 4) -995.127 0.16386819\n",
      "None (321, 321, 4) 1283.9999 0.0022644862\n",
      "None (321, 1536) -1017.8952 0.061281413\n"
     ]
    }
   ],
   "source": [
    "'''2. Multiplying the above attn scores with the values vector\n",
    "\n",
    "WARNING: even the stddev is the same, but the sum is off by a small value...\n",
    "\n",
    "context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "value_layer torch.Size([1, 4, 321, 384]) tensor(-995.1271, grad_fn=<SumBackward0>) tensor(0.1639, grad_fn=<StdBackward0>)\n",
    "attention_probs torch.Size([1, 4, 321, 321]) tensor(1283.9999, grad_fn=<SumBackward0>)\n",
    "context_layer torch.Size([1, 4, 321, 384]) tensor(-1023.2032, grad_fn=<SumBackward0>) tensor(0.0612, grad_fn=<StdBackward0>)\n",
    "'''\n",
    "# the _2 part is just dimension change; exported it just to check in case\n",
    "valueprint(MHAttention_v)\n",
    "valueprint(MHAttention_result_attn)\n",
    "valueprint(MHAttention_result_result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "auto attn = dropout(softmax(scores, 1), pDropout);\n",
    "arr = attn.array();\n",
    "af::saveArray(\"MHAttention_result_attn\", arr, savePathChar, true);\n",
    "\n",
    "auto result = matmul(attn.as(v.type()), v);\n",
    "\n",
    "arr = result.array();\n",
    "af::saveArray(\"MHAttention_result_result\", arr, savePathChar, true);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_v torch.Size([4, 321, 384]) tensor(-995.1271) tensor(0.1639)\n",
      "out_attn torch.Size([4, 321, 321]) tensor(1283.9999) tensor(0.0023)\n",
      "None torch.Size([4, 321, 384]) tensor(-1017.8952) tensor(0.0613)\n"
     ]
    }
   ],
   "source": [
    "out_v = torch.Tensor(MHAttention_v).transpose(1,2).transpose(0,1)\n",
    "out_attn = torch.Tensor(MHAttention_result_attn).transpose(1,2).transpose(0,1)\n",
    "out_result_result = torch.matmul(out_attn, out_v)\n",
    "\n",
    "valueprint(out_v, \"out_v\")\n",
    "valueprint(out_attn, \"out_attn\")\n",
    "valueprint(out_result_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selfattn_attention_probs torch.Size([1, 4, 321, 321]) tensor(1283.9999, grad_fn=<SumBackward0>)\n",
      "selfattn_attention_probs_HEADS torch.Size([1, 4, 321, 321]) tensor(1283.9999, grad_fn=<SumBackward0>)\n",
      "value_layer torch.Size([1, 4, 321, 384]) tensor(-995.1271, grad_fn=<SumBackward0>) tensor(0.1639, grad_fn=<StdBackward0>)\n",
      "context_layer torch.Size([1, 4, 321, 384]) tensor(-1023.2032, grad_fn=<SumBackward0>) tensor(0.0612, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# so it must be the case that the attn scores, despite being the sames sum, are wrong.\n",
    "our_attn = model.mctc.encoder.layers[0].attention.self\n",
    "our_output = torch.Tensor(layer_x).unsqueeze(0).transpose(1,2)\n",
    "result, attn, value = our_attn(our_output, output_attentions=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None torch.Size([321, 384]) tensor(-65.4505) tensor(0.0465)\n",
      "\n",
      "None torch.Size([321, 384]) tensor(-65.4504, grad_fn=<SumBackward0>) tensor(0.0465, grad_fn=<StdBackward0>)\n",
      "None torch.Size([321, 384]) tensor(-65.4505, grad_fn=<SumBackward0>) tensor(0.0465, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "fl_attn_scores = out_attn[0]\n",
    "fl_value = out_v[0]\n",
    "fl_result = out_result_result[0]\n",
    "\n",
    "our_attn_scores = attn.squeeze(0)[0] + 0.000013895\n",
    "our_value = value.squeeze(0)[0]\n",
    "\n",
    "our_manual = torch.matmul(our_attn_scores, our_value)\n",
    "our_manual_test_attn = torch.matmul(our_attn_scores, fl_value)\n",
    "our_manual_test_value = torch.matmul(fl_attn_scores, our_value)\n",
    "\n",
    "valueprint(fl_result)\n",
    "print()\n",
    "valueprint(our_manual)\n",
    "valueprint(our_manual_test_attn)\n",
    "# valueprint(our_manual_test_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None (321, 1536) -1017.8952 0.061281413\n",
      "None (1536, 321) 25.706522 0.1094709\n"
     ]
    }
   ],
   "source": [
    "'''3. wf_ multiply (SelfOUtput.dense) on above (attn*value) output\n",
    "\n",
    "SelfOUtput input torch.Size([1, 321, 1536]) tensor(-1023.2032, grad_fn=<SumBackward0>)\n",
    "SelfOUtput prenorm torch.Size([1, 321, 1536]) tensor(22.9749, grad_fn=<SumBackward0>)\n",
    "'''\n",
    "\n",
    "# selfAttention_result_2 = (*wf_)(transpose(selfAttention_result));\n",
    "valueprint(selfAttention_result)\n",
    "valueprint(selfAttention_result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "005f445f18c7a8bd0fe064b3495835c6fa643dca1e16cc09fc8a4180886f88ef"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('chanwookim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

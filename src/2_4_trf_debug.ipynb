{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing again after chaning MCTCSelfOutput to this differnt way flashlight seems to do things\n",
    "\n",
    "# testing after fixing the model & weights\n",
    "import torch\n",
    "from transformers import MCTCForCTC, MCTCProcessor, MCTCConfig\n",
    "config = MCTCConfig()\n",
    "model = MCTCForCTC(config)\n",
    "model.load_state_dict(torch.load(\"./ported_pytorch_model.bin\"))\n",
    "model = model.eval()\n",
    "our_self = model.mctc.encoder.layers[0].attention.self\n",
    "first_layer = model.mctc.encoder.layers[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 321, 1536]) INPUT\n",
      "attention_scores torch.Size([1, 4, 321, 321]) tensor(87683.4062, grad_fn=<SumBackward0>) tensor(0.9123, grad_fn=<StdBackward0>)\n",
      "value_layer torch.Size([1, 4, 321, 384]) tensor(-995.1271, grad_fn=<SumBackward0>) tensor(0.1639, grad_fn=<StdBackward0>)\n",
      "context_layer torch.Size([1, 4, 321, 384]) tensor(-1017.8952, grad_fn=<SumBackward0>) tensor(0.0613, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import arrayfire as af\n",
    "\n",
    "model = model.float()\n",
    "layer_x = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_x\").to_ndarray()\n",
    "\n",
    "def valueprint(k, name=None):\n",
    "    print(name, k.shape, k.sum(), k.std())\n",
    "our_output = torch.tensor(layer_x).unsqueeze(0).transpose(1,2)\n",
    "print(our_output.shape, \"INPUT\")\n",
    "\n",
    "context_layer, attention_scores, attention_probs, value_layer, save_dict = our_self(our_output, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores torch.Size([1, 4, 321, 321]) tensor(87683.4062, grad_fn=<SumBackward0>) tensor(0.9123, grad_fn=<StdBackward0>)\n",
      "value_layer torch.Size([1, 4, 321, 384]) tensor(-995.1271, grad_fn=<SumBackward0>) tensor(0.1639, grad_fn=<StdBackward0>)\n",
      "context_layer torch.Size([1, 4, 321, 384]) tensor(-1017.8952, grad_fn=<SumBackward0>) tensor(0.0613, grad_fn=<StdBackward0>)\n",
      "MHAttention_scores (321, 321, 4) 30256.422 0.9070506\n",
      "MHAttention_scores (321, 321, 4) 30256.418 0.9070506\n",
      "DIFF (321, 321, 4) 0.0004733706 2.862019e-07\n",
      "\n",
      "MHAttention_matmulPos (1839, 321, 4) -85057.48 0.13692527\n",
      "MHAttention_matmulPos (1839, 321, 4) -85057.41 0.13692527\n",
      "DIFF (1839, 321, 4) -5.647499e-06 4.3446907e-08\n",
      "\n",
      "MHAttention_pscores_transposed (321, 321, 4) 57426.992 0.19243532\n",
      "MHAttention_pscores_transposed (321, 321, 4) 57426.99 0.19243534\n",
      "DIFF (321, 321, 4) 1.8553577e-05 7.244098e-08\n",
      "\n",
      "MHAttention_pscores_scores (321, 321, 4) 87683.41 0.91226983\n",
      "MHAttention_pscores_scores (321, 321, 4) 87683.41 0.9122699\n",
      "DIFF (321, 321, 4) 0.00046512886 2.9637442e-07\n",
      "\n",
      "MHAttention_result (321, 321, 4) 87683.41 0.91226983\n",
      "MHAttention_result (321, 321, 4) 87683.41 0.9122699\n",
      "DIFF (321, 321, 4) 0.00046512886 2.9637442e-07\n",
      "\n",
      "MHAttention_result_attn (321, 321, 4) 1283.9999 0.0022644862\n",
      "MHAttention_result_attn (321, 321, 4) 1283.9999 0.0022644862\n",
      "DIFF (321, 321, 4) 3.5165094e-06 2.543757e-09\n",
      "\n",
      "MHAttention_v (321, 384, 4) -995.127 0.16386819\n",
      "MHAttention_v (321, 384, 4) -995.1271 0.16386819\n",
      "DIFF (321, 384, 4) -3.4673543e-05 4.1301018e-08\n",
      "\n",
      "MHAttention_result_result (321, 384, 4) -1017.8952 0.061281413\n",
      "MHAttention_result_result (321, 384, 4) -1017.8952 0.061281413\n",
      "DIFF (321, 384, 4) -9.098214e-06 2.5570774e-08\n",
      "\n",
      "MHAttention_result_result_2 (321, 1536) -1017.8952 0.061281413\n",
      "MHAttention_result_result_2 (321, 1536) -1017.89526 0.061281417\n",
      "DIFF (321, 1536) -9.098218e-06 2.557077e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Softmax of above final pscores\n",
    "'''\n",
    "our_self = model.mctc.encoder.layers[0].attention.self\n",
    "context_layer, attention_scores, attention_probs, value_layer, save_dict = our_self(our_output, output_attentions=True)\n",
    "mapper = {\n",
    "    \"MHAttention_scores\": save_dict[\"pre_posemb_attention_scores\"],\n",
    "    \"MHAttention_matmulPos\": save_dict[\"relative_position_scores_1\"],\n",
    "    \"MHAttention_pscores_transposed\": save_dict[\"relative_position_scores_2\"],\n",
    "    \"MHAttention_pscores_scores\": save_dict[\"attention_scores\"],\n",
    "    \"MHAttention_result\": save_dict[\"post_posemb_attention_scores\"],\n",
    "    \"MHAttention_result_attn\": save_dict[\"attention_probs\"],\n",
    "    # \"MHAttention_value\": save_dict[\"value_layer\"],\n",
    "    \"MHAttention_v\": save_dict[\"value_layer\"],\n",
    "    \"MHAttention_result_result\": save_dict[\"context_layer\"],\n",
    "    \"MHAttention_result_result_2\": context_layer\n",
    "}\n",
    "o2o = {}\n",
    "for key, myval in mapper.items():\n",
    "    fl_val = af.array.read_array(\"OUTPUT_TRF.arr\", key=key).to_ndarray()\n",
    "    o2o[key] = {\n",
    "        \"fl\": fl_val,\n",
    "        \"my\": myval,\n",
    "    }\n",
    "\n",
    "    valueprint(fl_val, key)\n",
    "    \n",
    "    # valueprint(myval.detach().numpy(), key)\n",
    "    if key == \"MHAttention_result_result_2\":\n",
    "        myval = myval.squeeze(0).detach().numpy()\n",
    "    else:\n",
    "        myval = myval.squeeze(0).permute(1,2,0).detach().numpy()\n",
    "    valueprint(myval, key)\n",
    "\n",
    "    diff = fl_val - myval\n",
    "    valueprint(diff, \"DIFF\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None (1536, 1536) -44.906384 0.017451134\n",
      "None torch.Size([1536, 1536]) tensor(-44.9064, grad_fn=<SumBackward0>) tensor(0.0175, grad_fn=<StdBackward0>)\n",
      "\n",
      "None (321, 1536) -1017.8952 0.061281413\n",
      "None torch.Size([1, 321, 1536]) tensor(-1017.8952, grad_fn=<SumBackward0>) tensor(0.0613, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "selfAttention_wf_ = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"selfAttention_wf_\").to_ndarray()\n",
    "selfAttention_result = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"selfAttention_result\").to_ndarray()\n",
    "GOLD_OUTPUT = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"selfAttention_result_2\").to_ndarray()\n",
    "my_wf = model.mctc.encoder.layers[0].attention.output.dense.weight\n",
    "my_result = context_layer\n",
    "\n",
    "valueprint(selfAttention_wf_)\n",
    "valueprint(my_wf)\n",
    "print()\n",
    "valueprint(selfAttention_result)\n",
    "valueprint(my_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None torch.Size([1536, 321]) tensor(25.7065, dtype=torch.float64) tensor(0.1095, dtype=torch.float64)\n",
      "None torch.Size([1, 1536, 321]) tensor(25.7065, dtype=torch.float64) tensor(0.1095, dtype=torch.float64)\n",
      "None torch.Size([1, 1536, 321]) tensor(18.9476, grad_fn=<SumBackward0>) tensor(0.0010, grad_fn=<StdBackward0>)\n",
      "None torch.Size([1, 321, 1536]) tensor(25.7065, grad_fn=<SumBackward0>) tensor(0.1095, grad_fn=<StdBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17571/2236328910.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hidden_states_gold = torch.matmul(torch.tensor(my_wf, dtype=torch.float64), torch.tensor(selfAttention_result, dtype=torch.float64).transpose(-1, -2))\n",
      "/tmp/ipykernel_17571/2236328910.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hidden_states_mine = torch.matmul(torch.tensor(my_wf, dtype=torch.float64), torch.tensor(my_result, dtype=torch.float64).transpose(-1, -2))\n"
     ]
    }
   ],
   "source": [
    "ll = torch.nn.Linear(1536, 1536, bias=False)\n",
    "ll.weight = torch.nn.Parameter(my_wf)\n",
    "hidden_states_gold = torch.matmul(torch.tensor(my_wf, dtype=torch.float64), torch.tensor(selfAttention_result, dtype=torch.float64).transpose(-1, -2))\n",
    "hidden_states_mine = torch.matmul(torch.tensor(my_wf, dtype=torch.float64), torch.tensor(my_result, dtype=torch.float64).transpose(-1, -2))\n",
    "hidden_states_einsum = torch.einsum('hh, bhe -> bhe', my_wf.float(), my_result.transpose(-1, -2))\n",
    "hideen_states_linear = ll(my_result)\n",
    "valueprint(hidden_states_gold)\n",
    "valueprint(hidden_states_mine)\n",
    "valueprint(hidden_states_einsum)\n",
    "valueprint(hideen_states_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores torch.Size([1, 4, 321, 321]) tensor(87683.4148, dtype=torch.float64, grad_fn=<SumBackward0>) tensor(0.9123, dtype=torch.float64, grad_fn=<StdBackward0>)\n",
      "value_layer torch.Size([1, 4, 321, 384]) tensor(-995.1271, dtype=torch.float64, grad_fn=<SumBackward0>) tensor(0.1639, dtype=torch.float64, grad_fn=<StdBackward0>)\n",
      "context_layer torch.Size([1, 4, 321, 384]) tensor(-1017.8953, dtype=torch.float64, grad_fn=<SumBackward0>) tensor(0.0613, dtype=torch.float64, grad_fn=<StdBackward0>)\n",
      "SelfOUtput prenorm torch.Size([1, 321, 1536]) tensor(25.7065, dtype=torch.float64, grad_fn=<SumBackward0>) tensor(0.1095, dtype=torch.float64, grad_fn=<StdBackward0>)\n",
      "SelfOUtput input_tensor torch.Size([1, 321, 1536]) tensor(330.5733, dtype=torch.float64)\n",
      "SelfOUtput postnorm torch.Size([1, 321, 1536]) tensor(-614.8109, dtype=torch.float64, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# testing again after chaning MCTCSelfOutput to this differnt way flashlight seems to do things\n",
    "\n",
    "# testing after fixing the model & weights\n",
    "import torch\n",
    "from transformers import MCTCForCTC, MCTCProcessor, MCTCConfig\n",
    "config = MCTCConfig()\n",
    "model = MCTCForCTC(config)\n",
    "model.load_state_dict(torch.load(\"./ported_pytorch_model.bin\"))\n",
    "model = model.eval()\n",
    "our_attn = model.mctc.encoder.layers[0].attention\n",
    "first_layer = model.mctc.encoder.layers[0]\n",
    "\n",
    "# lets take a deeper look.\n",
    "import arrayfire as af\n",
    "\n",
    "model = model.double()\n",
    "layer_x = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_x\").to_ndarray()\n",
    "\n",
    "def valueprint(k, name=None):\n",
    "    print(name, k.shape, k.sum(), k.std())\n",
    "our_output = torch.tensor(layer_x, dtype=torch.float64).unsqueeze(0).transpose(1,2)\n",
    "context_layer, attention_scores, attention_probs, value_layer, save_dict = our_attn(our_output, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None (1536, 321) 25.706522 0.1094709\n"
     ]
    }
   ],
   "source": [
    "GOLD_OUTPUT = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_selfAttnResult\").to_ndarray()\n",
    "valueprint(GOLD_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None (1536, 321) 330.57333 0.15868725\n",
      "None (1536, 321) 25.706522 0.1094709\n",
      "None (1536, 321) -1035.2311 1.179761\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "af::saveArray(\"layer_selfAttnResult\", input_arr, savePathChar, true);\n",
    "\n",
    "auto h = (*norm1_)((f * selfAttnResult.as(x.type())) + x);\n",
    "\n",
    "af::saveArray(\"layer_h\", input_arr, savePathChar, true); \n",
    "'''\n",
    "layer_x = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_x\").to_ndarray()\n",
    "GOLD_OUTPUT_selfAttn = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_selfAttnResult\").to_ndarray()\n",
    "GOLD_OUTPUT_postnorm = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_h\").to_ndarray()\n",
    "valueprint(layer_x)\n",
    "valueprint(GOLD_OUTPUT_selfAttn)\n",
    "valueprint(GOLD_OUTPUT_postnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None torch.Size([321, 1536]) tensor(330.5734) tensor(0.1587)\n",
      "None torch.Size([321, 1536]) tensor(25.7066) tensor(0.1095)\n",
      "None torch.Size([321, 1536]) tensor(-614.8109, dtype=torch.float64, grad_fn=<SumBackward0>) tensor(0.2113, dtype=torch.float64, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "my_x = torch.Tensor(layer_x).transpose(0,1)\n",
    "my_selfAttnResult = torch.Tensor(GOLD_OUTPUT_selfAttn).transpose(0,1)\n",
    "\n",
    "output_norm = model.mctc.encoder.layers[0].attention.output.LayerNorm\n",
    "\n",
    "valueprint(my_x)\n",
    "valueprint(my_selfAttnResult)\n",
    "\n",
    "my_sum = my_selfAttnResult.transpose(-1, -2) + my_x.transpose(-1, -2)\n",
    "my_sum = my_sum.transpose(-1, -2)\n",
    "my_postnorm = output_norm(my_sum)\n",
    "valueprint(my_postnorm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None (1,) 1.180032 0.0\n",
      "None (1,) -0.0020996246 0.0\n",
      "None torch.Size([1]) tensor(1.1800, dtype=torch.float64, grad_fn=<SumBackward0>) tensor(nan, dtype=torch.float64, grad_fn=<StdBackward0>)\n",
      "None torch.Size([1]) tensor(-0.0021, dtype=torch.float64, grad_fn=<SumBackward0>) tensor(nan, dtype=torch.float64, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_norm1_w = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_norm1_w\").to_ndarray()\n",
    "layer_norm1_b = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_norm1_b\").to_ndarray()\n",
    "\n",
    "valueprint(layer_norm1_w)\n",
    "valueprint(layer_norm1_b)\n",
    "\n",
    "valueprint(output_norm.singleton_weight)\n",
    "valueprint(output_norm.singleton_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layernorm = torch.nn.LayerNorm((1536), eps=1e-05, elementwise_affine=True, device=None, dtype=None)\n",
    "layernorm.weight = torch.nn.Parameter(output_norm.singleton_weight.float().tile((1536,)))\n",
    "layernorm.bias = torch.nn.Parameter(output_norm.singleton_bias.float().tile((1536,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None torch.Size([321, 1536]) tensor(-1035.2329, grad_fn=<SumBackward0>) tensor(1.1798, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "my_postnorm = layernorm(my_sum)\n",
    "valueprint(my_postnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores torch.Size([1, 4, 321, 321]) tensor(87683.4148, dtype=torch.float64, grad_fn=<SumBackward0>) tensor(0.9123, dtype=torch.float64, grad_fn=<StdBackward0>)\n",
      "value_layer torch.Size([1, 4, 321, 384]) tensor(-995.1271, dtype=torch.float64, grad_fn=<SumBackward0>) tensor(0.1639, dtype=torch.float64, grad_fn=<StdBackward0>)\n",
      "context_layer torch.Size([1, 4, 321, 384]) tensor(-1017.8953, dtype=torch.float64, grad_fn=<SumBackward0>) tensor(0.0613, dtype=torch.float64, grad_fn=<StdBackward0>)\n",
      "SelfOUtput prenorm torch.Size([1, 321, 1536]) tensor(25.7065, dtype=torch.float64, grad_fn=<SumBackward0>) tensor(0.1095, dtype=torch.float64, grad_fn=<StdBackward0>)\n",
      "SelfOUtput input_tensor torch.Size([1, 321, 1536]) tensor(330.5733, dtype=torch.float64)\n",
      "SelfOUtput postnorm torch.Size([1, 321, 1536]) tensor(-1035.2325, dtype=torch.float64, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# adjusted the weights accordingly\n",
    "# testing again after chaning MCTCSelfOutput to this differnt way flashlight seems to do things\n",
    "\n",
    "# testing after fixing the model & weights\n",
    "import torch\n",
    "from transformers import MCTCForCTC, MCTCProcessor, MCTCConfig\n",
    "config = MCTCConfig()\n",
    "model = MCTCForCTC(config)\n",
    "model.load_state_dict(torch.load(\"./ported_pytorch_model.bin\"))\n",
    "model = model.eval()\n",
    "our_attn = model.mctc.encoder.layers[0].attention\n",
    "first_layer = model.mctc.encoder.layers[0]\n",
    "\n",
    "# lets take a deeper look.\n",
    "import arrayfire as af\n",
    "\n",
    "model = model.double()\n",
    "layer_x = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_x\").to_ndarray()\n",
    "\n",
    "def valueprint(k, name=None):\n",
    "    print(name, k.shape, k.sum(), k.std())\n",
    "our_output = torch.tensor(layer_x, dtype=torch.float64).unsqueeze(0).transpose(1,2)\n",
    "context_layer, attention_scores, attention_probs, value_layer, save_dict = our_attn(our_output, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None torch.Size([1, 321, 1536]) tensor(-1035.2325, dtype=torch.float64, grad_fn=<SumBackward0>) tensor(1.1798, dtype=torch.float64, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "valueprint(context_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None torch.Size([1, 321, 1536]) tensor(14736.9198, dtype=torch.float64, grad_fn=<SumBackward0>) tensor(0.4444, dtype=torch.float64, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "valueprint(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None (1536, 321) 14736.917 0.4444032\n"
     ]
    }
   ],
   "source": [
    "layer_h_2 = af.array.read_array(\"OUTPUT_TRF.arr\", key=\"layer_h_2\").to_ndarray()\n",
    "valueprint(layer_h_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [3072, 80, 7], expected input[1, 1536, 327] to have 80 channels, but got 1536 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/chanwookim/transformers/src/2_4_trf_debug.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://a100.1theta.com/home/ubuntu/chanwookim/transformers/src/2_4_trf_debug.ipynb#ch0000017vscode-remote?line=0'>1</a>\u001b[0m full_output \u001b[39m=\u001b[39m model(our_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py:989\u001b[0m, in \u001b[0;36mMCTCForCTC.forward\u001b[0;34m(self, input_features, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=979'>980</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=980'>981</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=981'>982</a>\u001b[0m \u001b[39m    Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=984'>985</a>\u001b[0m \u001b[39m    config.vocab_size - 1]`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=985'>986</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=987'>988</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=988'>989</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmctc(\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=989'>990</a>\u001b[0m     input_features,\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=990'>991</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=991'>992</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=992'>993</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=993'>994</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=994'>995</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=996'>997</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=998'>999</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctc_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py:918\u001b[0m, in \u001b[0;36mMCTCModel.forward\u001b[0;34m(self, input_features, attention_mask, head_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=914'>915</a>\u001b[0m \u001b[39mif\u001b[39;00m input_features \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=915'>916</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify input_features.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=917'>918</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=918'>919</a>\u001b[0m     input_features,\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=919'>920</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=920'>921</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=921'>922</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=922'>923</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=923'>924</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=924'>925</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=925'>926</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=927'>928</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py:790\u001b[0m, in \u001b[0;36mMCTCEncoder.forward\u001b[0;34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=785'>786</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=787'>788</a>\u001b[0m input_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(input_features)\n\u001b[0;32m--> <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=789'>790</a>\u001b[0m inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(input_features)\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=791'>792</a>\u001b[0m \u001b[39m# inputs_embeds = self.embed_scale * inputs_embeds\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=792'>793</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=793'>794</a>\u001b[0m \u001b[39m# subsample attention mask if necessary\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=794'>795</a>\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py:204\u001b[0m, in \u001b[0;36mConv1dSubsampler.forward\u001b[0;34m(self, input_features)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=201'>202</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m input_features\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()  \u001b[39m# -> B x F x T\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=202'>203</a>\u001b[0m \u001b[39mfor\u001b[39;00m conv \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_layers:\n\u001b[0;32m--> <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=203'>204</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m conv(hidden_states)\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=204'>205</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mglu(hidden_states, dim\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglu_dim)\n\u001b[1;32m    <a href='file:///home/ubuntu/chanwookim/transformers/src/transformers/models/mctc/modeling_mctc.py?line=205'>206</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/conv.py:302\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=300'>301</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=301'>302</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/conv.py:298\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=293'>294</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=294'>295</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=295'>296</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=296'>297</a>\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=297'>298</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/chanwookim/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=298'>299</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [3072, 80, 7], expected input[1, 1536, 327] to have 80 channels, but got 1536 channels instead"
     ]
    }
   ],
   "source": [
    "full_output = model(our_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "005f445f18c7a8bd0fe064b3495835c6fa643dca1e16cc09fc8a4180886f88ef"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('chanwookim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
